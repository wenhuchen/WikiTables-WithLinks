{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling data code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_table.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "import sys\n",
    "import requests\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import os\n",
    "\n",
    "print('original data has {} entries'.format(len(data)))\n",
    "new_data = []\n",
    "for i, d in enumerate(data):\n",
    "    sys.stdout.write(\"finished {}/{} \\r\".format(i, len(data)))\n",
    "    title = d['title']\n",
    "    title = '_'.join(title.split(' '))    \n",
    "    page = 'https://en.wikipedia.org/wiki/{}'.format(title)\n",
    "        \n",
    "    if len(d['data']) > 5 and len(d['data']) < 40 and len(d['data'][0]) >= 4:\n",
    "        headers = set(d['header'])\n",
    "        if len(headers) == len(d['header']):\n",
    "            cols = len(d['header'])\n",
    "            count = 0\n",
    "            for g in d['data'][0]:\n",
    "                if g[1] is not None:\n",
    "                    count += 1\n",
    "            if count < 0.3 * cols:\n",
    "                continue\n",
    "            \n",
    "            # process if there are enough hyperlinks\n",
    "            title = d['title']\n",
    "            title = '_'.join(title.split(' '))\n",
    "                \n",
    "            if not os.path.exists('htmls/{}.html'.format(title)):\n",
    "                try:\n",
    "                    response = urllib.request.urlopen(page)\n",
    "                    webContent = response.read()\n",
    "                    f = open('htmls/{}.html'.format(title), 'wb')\n",
    "                    f.write(webContent)\n",
    "                    f.close()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            d['page'] = '{}.html'.format(title)\n",
    "            new_data.append(d)\n",
    "\n",
    "print('filtered data has {} entries'.format(len(new_data)))\n",
    "with open('processed_table_with_page.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "\n",
    "def process_link(text):\n",
    "    tmp = []\n",
    "    hrefs = []\n",
    "    for t in text.find_all('a'):\n",
    "        if len(t.get_text().strip()) > 0:\n",
    "            if 'href' in t.attrs and t['href'].startswith('/wiki/'):\n",
    "                tmp.append(t.get_text().strip())\n",
    "                hrefs.append(t['href'])\n",
    "            else:\n",
    "                tmp.append(t.get_text().strip())\n",
    "                hrefs.append('#')\n",
    "    if all([_ == '#' for _ in hrefs]):\n",
    "        return ','.join(tmp).strip(), None\n",
    "    else:\n",
    "        return ','.join(tmp).strip(), ' '.join(hrefs)\n",
    "\n",
    "def remove_ref(text):\n",
    "    for x in text.find_all('sup'):\n",
    "        x.extract()\n",
    "    return text\n",
    "\n",
    "files = os.listdir('htmls/')\n",
    "\n",
    "def sub_func(f_name):\n",
    "    results = []\n",
    "    with open('htmls/' + f_name, 'r') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        rs = soup.find_all(class_='wikitable sortable')\n",
    "        \n",
    "        for r in rs:\n",
    "            heads = []\n",
    "            rows = []\n",
    "            for i, t_row in enumerate(r.find_all('tr')):\n",
    "                if i == 0:\n",
    "                    for h in t_row.find_all(['th', 'td']):\n",
    "                        h = remove_ref(h)\n",
    "                        if len(h.find_all('a')) > 0:\n",
    "                            heads.append(process_link(h))\n",
    "                        else:\n",
    "                            heads.append((h.get_text().strip(), None))\n",
    "                else:\n",
    "                    row = []\n",
    "                    for h in t_row.find_all(['th', 'td']):\n",
    "                        h = remove_ref(h)\n",
    "                        if len(h.find_all('a')) > 0:\n",
    "                            row.append(process_link(h))\n",
    "                        else:\n",
    "                            row.append((h.get_text().strip(), None))\n",
    "                    if all([len(cell[0]) == 0 for cell in row]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        rows.append(row)\n",
    "            \n",
    "            rows = rows[:20]\n",
    "            if any([len(row) != len(heads) for row in rows]) or len(rows) < 8:\n",
    "                continue\n",
    "            else:\n",
    "                text = r.previous_sibling\n",
    "                while text is not None and text.name != 'p' and text.string is not None:\n",
    "                    text = text.previous_sibling\n",
    "\n",
    "                if text is None or text.string is None:\n",
    "                    context = ''\n",
    "                else:\n",
    "                    context = text.string.strip()\n",
    "\n",
    "                title = soup.title.string\n",
    "                title = re.sub(' - Wikipedia', '', title)\n",
    "                url = 'https://en.wikipedia.org/wiki/{}'.format('_'.join(title.split(' ')))\n",
    "                results.append({'url': url, 'title': title, 'header': heads, 'data': rows, 'context': context})\n",
    "    return results\n",
    "\n",
    "rs = []\n",
    "for f in files:\n",
    "    tmp = sub_func(f)\n",
    "    print(tmp)\n",
    "    rs.append(tmp)\n",
    "\n",
    "results = []\n",
    "for r in rs:\n",
    "    results = results + r\n",
    "\n",
    "with open('processed_new_table.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_new_table.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "#print(\"there are {} tables in total\".format(len(tables)))\n",
    "\n",
    "deletes = []\n",
    "for i, table in enumerate(tables):\n",
    "    # Remove sparse columns\n",
    "    to_remove = []\n",
    "    for j, h in enumerate(table['header']):\n",
    "        #if j == 0:\n",
    "        #    continue\n",
    "        if 'Coordinates' in h[0][0] or 'Image' in h[0][0]:\n",
    "            to_remove.append(j)\n",
    "            continue\n",
    "        \n",
    "        count = 0\n",
    "        total = len(table['data'])\n",
    "        for d in table['data']:\n",
    "            #print(d[j])\n",
    "            if d[j][0][0] != '':\n",
    "                count += 1\n",
    "        \n",
    "        if count / total < 0.5:\n",
    "            to_remove.append(j)\n",
    "    \n",
    "    bias = 0\n",
    "    for r in to_remove:\n",
    "        del tables[i]['header'][r - bias]\n",
    "        for _ in range(len(table['data'])):\n",
    "            del tables[i]['data'][_][r - bias]\n",
    "        bias += 1\n",
    "    \n",
    "    # Remove sparse rows\n",
    "    to_remove = []\n",
    "    for k in range(len(table['data'])):\n",
    "        non_empty = [1 if _[0][0] != '' else 0 for _ in table['data'][k]]\n",
    "        if sum(non_empty) < 0.5 * len(non_empty):\n",
    "            to_remove.append(k)\n",
    "    \n",
    "    bias = 0\n",
    "    for r in to_remove:        \n",
    "        del tables[i]['data'][r - bias]\n",
    "        bias += 1\n",
    "    \n",
    "    if len(table['header']) > 6:\n",
    "        deletes.append(i)\n",
    "    elif len(table['header']) <= 2:\n",
    "        deletes.append(i)\n",
    "    else:\n",
    "        count = 0\n",
    "        total = 0\n",
    "        for row in table['data']:\n",
    "            for cell in row:\n",
    "                if len(cell[0][0]) != '':\n",
    "                    if cell[1] == [None]:\n",
    "                        count += 1                    \n",
    "                    total += 1\n",
    "        if count / total >= 0.7:\n",
    "            deletes.append(i)\n",
    "\n",
    "print('out of {} tables, {} need to be deleted'.format(len(tables), len(deletes)))\n",
    "\n",
    "bias = 0\n",
    "for i in deletes:\n",
    "    del tables[i - bias]\n",
    "    bias += 1\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'w') as f:\n",
    "    json.dump(tables, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas\n",
    "import json\n",
    "from yattag import Doc\n",
    "from yattag import indent\n",
    "import random\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "doc, tag, text = Doc().tagtext()\n",
    "\n",
    "cache = ''\n",
    "\n",
    "style = \"\"\"\n",
    "    th {\n",
    "        padding-top: 12px;\n",
    "        padding-bottom: 12px;\n",
    "        text-align: left;\n",
    "        background-color: #c9c9c9;\n",
    "        color: black;\n",
    "    }\n",
    "    td, th {\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }\n",
    "    td {\n",
    "        padding-top: 12px;\n",
    "        padding-bottom: 12px;\n",
    "        text-align: left;\n",
    "        background-color: #f0f0f0;\n",
    "        color: black;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "doc.asis('<!DOCTYPE html>')\n",
    "with tag('html'):\n",
    "    with tag('head'):\n",
    "        with tag('style'):\n",
    "            doc.asis(style)\n",
    "\n",
    "        doc.asis('<meta charset=\\\"utf-8\\\">')\n",
    "        doc.asis('<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\">')\n",
    "        doc.asis('<title>Demonstration</title>')\n",
    "        doc.asis('<link rel=\"icon\" href=\"\">') #Modifier ici pour le favicon\n",
    "        doc.asis('<script defer src=\"https://use.fontawesome.com/releases/v5.3.1/js/all.js\"></script>')\n",
    "    \n",
    "    \n",
    "    with tag('body'):\n",
    "        random.shuffle(tables)\n",
    "        for table in tables[:200]:\n",
    "            with tag('h3'):\n",
    "                with tag('a', href=table['url']):\n",
    "                    text(table['title'])\n",
    "            \n",
    "            with tag('h4'):\n",
    "                text(table['context'])\n",
    "                \n",
    "            with tag('table', klass='wikitable', style=\"border:1\"):\n",
    "                with tag('tbody'):\n",
    "                    with tag('tr'):\n",
    "                        for cell in table['header']:\n",
    "                            with tag('th'):\n",
    "                                if cell[1] is not None:\n",
    "                                    count = 0\n",
    "                                    for t, s in zip(cell[0], cell[1]):\n",
    "                                        if s is not None:\n",
    "                                            with tag('a', href='http://edward.cs.ucsb.edu:6007/query?name=' + s):\n",
    "                                                text(t)\n",
    "                                            if count < len(cell[1]) - 1:\n",
    "                                                text(', ')\n",
    "                                        else:\n",
    "                                            text(t)\n",
    "                                            \n",
    "                                        count += 1\n",
    "                                else:\n",
    "                                    text(cell[0])\n",
    "\n",
    "                    for row in table['data']:\n",
    "                        with tag('tr'):\n",
    "                            for cell in row:\n",
    "                                with tag('td'):\n",
    "                                    if cell[1] is not None:\n",
    "                                        count = 0\n",
    "                                        for t, s in zip(cell[0], cell[1]):\n",
    "                                            if s is not None:\n",
    "                                                with tag('a', href='http://edward.cs.ucsb.edu:6007/query?name=' + s):\n",
    "                                                    text(t)\n",
    "                                                if count < len(cell[1]) - 1:\n",
    "                                                    text(', ')\n",
    "                                            else:\n",
    "                                                text(t)\n",
    "                                    else:\n",
    "                                        text(cell[0])\n",
    "            doc.stag('br')\n",
    "\n",
    "result = doc.getvalue()\n",
    "\n",
    "with open('index.html', 'w') as f:\n",
    "    f.write(indent(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(text):\n",
    "    if 'Section::::' in text:\n",
    "        text = text[:text.find('Section::::')]\n",
    "    try:\n",
    "        intro = text.split('\\n\\n')[1]\n",
    "        d = BeautifulSoup(intro)\n",
    "        intro = d.get_text().strip()\n",
    "        return intro\n",
    "    except Exception:\n",
    "        return 'N/A'\n",
    "\n",
    "dictionary = {}\n",
    "with open('en.json') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d = json.loads(line.strip())\n",
    "        page = '_'.join(d['title'].split(' '))\n",
    "        dictionary[page] = get_text(d['text'])\n",
    "        sys.stdout.write('finished {}/5989879 \\r'.format(i))\n",
    "        \n",
    "with open('wiki-intro-with-ents-dict.json', 'w') as f:\n",
    "    json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "dictionary = {}\n",
    "missed = []\n",
    "\n",
    "succ, fail = 0, 0\n",
    "for table in tables:\n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            pages = cell[1]\n",
    "            if pages is not None:\n",
    "                for page in pages.split(' '):\n",
    "                    page = page[6:].split('#')[0]\n",
    "                    if page not in database:\n",
    "                        fail += 1\n",
    "                        print(page)\n",
    "                        #database[page]\n",
    "                    else:\n",
    "                        succ += 1\n",
    "\n",
    "sys.stdout.write('success/fail = {}/{} \\r'.format(succ, fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "def get_summary(page):\n",
    "    if page.startswith('https'):\n",
    "        pass\n",
    "    elif page.startswith('/wiki'):\n",
    "        page = 'https://en.wikipedia.org{}'.format(page)\n",
    "    else:\n",
    "        page = 'https://en.wikipedia.org/wiki/{}'.format(page)\n",
    "    \n",
    "    r = http.request('GET', page)\n",
    "    if r.status == 200:\n",
    "        data = r.data.decode('utf-8')\n",
    "        data = data.replace('</p><p>', ' ')        \n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        div = soup.body.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "\n",
    "        children = div.findChildren(\"p\" , recursive=False)\n",
    "        summary = 'N/A'\n",
    "        for child in children:\n",
    "            if child.get_text().strip() != \"\":\n",
    "                html = str(child)\n",
    "                html = html[html.index('>') + 1:].strip()\n",
    "                if not html.startswith('<'):\n",
    "                    summary = child.get_text().strip()\n",
    "                    break\n",
    "                elif html.startswith('<a>') or html.startswith('<b>') or \\\n",
    "                        html.startswith('<i>') or html.startswith('<a ') or html.startswith('<br>'):\n",
    "                    summary = child.get_text().strip()\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        return summary\n",
    "    elif r.status == 429:\n",
    "        time.sleep(1)\n",
    "        return get_summary(page)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "get_summary('/wiki/Soe_Myint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dictionary = {}\n",
    "for f in os.listdir('hyperlinks/'):\n",
    "    if f.endswith('json'):\n",
    "        with open('hyperlinks/' + f, 'r') as fw:\n",
    "            d = json.load(fw)\n",
    "            dictionary.update(d)\n",
    "\n",
    "print('totally {}'.format(len(dictionary)))\n",
    "\n",
    "failed = [k for k, v in dictionary.items() if v == 'N/A']\n",
    "print('failed {} items'.format(len(failed)))\n",
    "\n",
    "with open('wikipedia/round1.json', 'w') as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "    \n",
    "with open('wikipedia/round1_failed.json', 'w') as f:\n",
    "    json.dump(failed, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia/round1.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "    \n",
    "with open('wikipedia/round2.json') as f:\n",
    "    dictionary.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(dictionary))\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "#for k, v in dictionary.items():\n",
    "#    dictionary[k] = re.sub(r'\\[[\\d]+\\]', '', v).strip()\n",
    "with open('wikipedia/merged.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "merged_unquote = {}\n",
    "for k, v in dictionary.items():\n",
    "    merged_unquote[urllib.parse.unquote(k)] = v\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'w') as f:\n",
    "    json.dump(merged_unquote, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    d['idx'] = i\n",
    "    with open('tables/{}.json'.format(i), 'w') as f:\n",
    "        json.dump(d, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Generating the request data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from utils import *\n",
    "import re\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "def recover(string):\n",
    "    string = string[6:]\n",
    "    string = string.replace('_', ' ')\n",
    "    return string\n",
    "    \n",
    "def clean_text(k, string):\n",
    "    if \"Initial visibility\" in string:\n",
    "        return recover(k)\n",
    "    \n",
    "    position = string.find(\"mw-parser-output\")\n",
    "    if position != -1:\n",
    "        left_quote = position - 1\n",
    "        while left_quote >= 0 and string[left_quote] != '(':\n",
    "            left_quote -= 1\n",
    "        right_quote = position + 1\n",
    "        while right_quote < len(string) and string[right_quote] != ')':\n",
    "            right_quote += 1\n",
    "        \n",
    "        string = string[:left_quote] + \" \" + string[right_quote + 1:]\n",
    "        \n",
    "        position = string.find(\"mw-parser-output\")\n",
    "        if position != -1:\n",
    "            #print(string)\n",
    "            right_quote = position + 1\n",
    "            while right_quote < len(string) and string[right_quote] != '\\n':\n",
    "                right_quote += 1\n",
    "            #print(\"----------------\")\n",
    "            string = string[:position] + string[right_quote + 1:]\n",
    "            #print(string)\n",
    "            #print(\"################\")\n",
    "    \n",
    "    string = string.replace(u'\\xa0', u' ')\n",
    "    string = string.replace('\\ufeff', '')\n",
    "    string = string.replace(u'\\u200e', u' ')\n",
    "    string = string.replace('â€“', '-')\n",
    "    string = string.replace(u'\\u2009', u' ')\n",
    "    string = string.replace(u'\\u2010', u' - ')\n",
    "    string = string.replace(u'\\u2011', u' - ')\n",
    "    string = string.replace(u'\\u2012', u' - ')\n",
    "    string = string.replace(u'\\u2013', u' - ')\n",
    "    string = string.replace(u'\\u2014', u' - ')\n",
    "    string = string.replace(u'\\u2015', u' - ')\n",
    "    string = string.replace(u'\\u2018', u'')\n",
    "    string = string.replace(u'\\u2019', u'')\n",
    "    string = string.replace(u'\\u201c', u'')\n",
    "    string = string.replace(u'\\u201d', u'')    \n",
    "    \n",
    "    string = string.replace(u'\"', u'')\n",
    "    string = re.sub(r'[\\n]+', '\\n', string)\n",
    "    \n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    \n",
    "    #string = re.sub(r\"'+\", \"'\", string)\n",
    "    #string = string.replace(\" '\", \" \")\n",
    "    #string = string.replace(\"' \", \" \")\n",
    "    string = filter_firstKsents(string, 12)\n",
    "    \n",
    "    return string\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'r') as f:\n",
    "    merged_unquote = json.load(f)\n",
    "\n",
    "for k in merged_unquote:\n",
    "    merged_unquote[k] = clean_text(k, merged_unquote[k])\n",
    "\n",
    "def func(f_id):\n",
    "    if f_id.endswith('.json'):\n",
    "        with open('tables/' + f_id) as f:\n",
    "            table = json.load(f)\n",
    "    \n",
    "    local_dict = {}\n",
    "    for d in table['header']:\n",
    "        for url in d[1]:\n",
    "            if url:\n",
    "                url = urllib.parse.unquote(url)\n",
    "                local_dict[url] = merged_unquote[url]\n",
    "    \n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            for url in cell[1]:\n",
    "                if url:\n",
    "                    url = urllib.parse.unquote(url)\n",
    "                    local_dict[url] = merged_unquote[url]\n",
    "    #count += 1\n",
    "    #sys.stdout.write(\"finished {} tables \\r\".format(count))\n",
    "    with open('request_wo_filter/{}'.format(f_id), 'w') as f:\n",
    "        json.dump(local_dict, f, indent=2)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(64)\n",
    "results_func = pool.map(func, os.listdir('tables/'))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "copyfile('request/example.json', 'request_wo_filter/examples.json')\n",
    "copyfile('request/example_numeric.json', 'request_wo_filter/example_numeric.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import *\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:5')\n",
    "\n",
    "pretrained_weights = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with open('Mixed-Reasoning/collected_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for d in data:\n",
    "    table_id = d[0]\n",
    "    with open('request/{}.json'.format(table_id)) as f:\n",
    "        requested_documents = json.load(f)\n",
    "    \n",
    "    idx2key = []\n",
    "    tmp = []\n",
    "    for k, v in requested_documents.items():\n",
    "        tokenized_paragraph = tokenizer.tokenize(v)\n",
    "        if len(tokenized_paragraph) < 512:\n",
    "            tokenized_paragraph = tokenized_paragraph[:512]\n",
    "        \n",
    "        idxs = tokenizer.convert_tokens_to_ids(tokenized_paragraph)\n",
    "        tensor = torch.LongTensor(idxs).unsqueeze(0).to(device)\n",
    "        _, r2 = model(tensor)\n",
    "        tmp.append(r2)\n",
    "        idx2key.append(k)\n",
    "    \n",
    "    requested_repr = torch.cat(tmp, 0)\n",
    "    for q, a in d[1:]:\n",
    "        idxs = tokenizer.encode(q)\n",
    "        print(idxs)\n",
    "        tensor = torch.LongTensor(idxs).unsqueeze(0).to(device)\n",
    "        _, r2 = model(tensor)\n",
    "        r2 = r2.repeat(requested_repr.shape[0], 1)\n",
    "        \n",
    "        similarity = torch.nn.functional.cosine_similarity(r2, requested_repr, dim=1)\n",
    "        print(similarity)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import nltk.data\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "from utils import *\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import dateparser\n",
    "from dateparser.search import search_dates\n",
    "from dateparser import parse\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "tfidf = TfidfVectorizer(strip_accents=\"unicode\", ngram_range=(2, 3), stop_words=stopWords)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "def longestSubstringFinder(S,T):\n",
    "    S = S.lower()\n",
    "    T = T.lower()\n",
    "    m = len(S)\n",
    "    n = len(T)\n",
    "    counter = [[0]*(n+1) for x in range(m+1)]\n",
    "    longest = 0\n",
    "    lcs_set = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if S[i] == T[j]:\n",
    "                c = counter[i][j] + 1\n",
    "                counter[i+1][j+1] = c\n",
    "                if c > longest:\n",
    "                    lcs_set = set()\n",
    "                    longest = c\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "                elif c == longest:\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "    \n",
    "    return longest, lcs_set\n",
    "\n",
    "def longest_match_distance(str1s, str2s):\n",
    "    longest_string = []\n",
    "    for str1 in str1s:\n",
    "        longest_string.append([])\n",
    "        for str2 in str2s:\n",
    "            length, _ = longestSubstringFinder(str1, str2)\n",
    "            longest_string[-1].append(1 - length / len(str1))\n",
    "    return longest_string\n",
    "\n",
    "def searchForAnswer(answer, table, passages, mapping_entity):\n",
    "    results = []\n",
    "    correction = None\n",
    "    for i, row in enumerate(table['data']):\n",
    "        for j, cell in enumerate(row):\n",
    "            success = False\n",
    "            for content, url in zip(cell[0], cell[1]):\n",
    "                if answer.lower() == content.lower():\n",
    "                    results.append((content, (i, j), url, 'table'))\n",
    "                    success = True\n",
    "                    break\n",
    "                elif \" \" + answer.lower() + \" \" in \" \" + content.lower() + \" \":\n",
    "                    correction = content\n",
    "                    results.append((content, (i, j), url, 'table'))\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            if not success and len(cell[0]) > 1:\n",
    "                content = ' , '.join(cell[0])\n",
    "                if answer == content:\n",
    "                    results.append((content, (i, j), None, 'table'))\n",
    "                elif len(answer) > 3 and \" \" + answer.lower() + \" \" in \" \" + content.lower() + \" \":\n",
    "                    correction = content\n",
    "                    results.append((content, (i, j), None, 'table'))\n",
    "\n",
    "    if len(results) > 0:\n",
    "        return correction, results\n",
    "        \n",
    "    for k, v in passages.items():\n",
    "        if \" \" + answer.lower() + \" \" in \" \" + v.lower() + \" \":\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    results.append((content, loc, k, 'passage'))\n",
    "    \n",
    "    return None, results\n",
    "\n",
    "def searchForAnswerWithoutSpace(answer, passages, mapping_entity):\n",
    "    correction = None\n",
    "    results = []\n",
    "    for k, v in passages.items():\n",
    "        tmp = (\" \" + v.lower()).find(\" \" + answer.lower())\n",
    "        if tmp != -1:\n",
    "            length = len(answer)\n",
    "            while tmp + length < len(v) and v[tmp + length] != \" \":\n",
    "                length += 1\n",
    "            correction = v[tmp:tmp + length]\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    results.append((content, loc, k, 'passage'))\n",
    "            break\n",
    "\n",
    "    return correction, results\n",
    "\n",
    "def get_edit_distance_equal_1(answer, table):\n",
    "    results = []\n",
    "    for i, row in enumerate(table['data']):\n",
    "        for j, cell in enumerate(row):\n",
    "            for tmp, url in zip(cell[0], cell[1]):\n",
    "                dist = nltk.edit_distance(answer, tmp)\n",
    "                if dist == 1:\n",
    "                    results.append((tmp, (i, j), url, 'table'))\n",
    "    return results\n",
    "\n",
    "def fixing_answer(string):\n",
    "    if ',' in string:\n",
    "        tmp = string.split(',')[0].strip()\n",
    "        if not tmp.isdigit():\n",
    "            string = [tmp]\n",
    "        else:\n",
    "            return None\n",
    "    elif ' and ' in string:\n",
    "        string = [_.strip() for _ in string.split(' and ')]\n",
    "    elif '(' and ')' in string:\n",
    "        tmp = re.sub(r'([^\\(\\)]+) \\((.+)\\)$', r'\\1###\\2', string)\n",
    "        string = [_.strip() for _ in tmp.split('###')]\n",
    "    elif '-' in string:\n",
    "        if ' - ' in string:\n",
    "            tmp = string.replace(' - ', '-')\n",
    "        else:\n",
    "            tmp = string.replace('-', ' - ')\n",
    "        string = [tmp, string.split('-')[0].strip(), string.split('-')[1].strip()]\n",
    "    elif string.startswith('#'):\n",
    "        string = [string.lstrip('#').strip()]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return string\n",
    "        \n",
    "def step1(d):\n",
    "    results = []\n",
    "    table_id = d[0]\n",
    "    #if table_id != 12040:\n",
    "    #    return []\n",
    "\n",
    "    with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "        requested_documents = json.load(f)\n",
    "    with open('tables_tok/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)    \n",
    "    \n",
    "    #title_wiki = table['url'][len('https://en.wikipedia.org'):]\n",
    "    #del requested_documents[title_wiki]\n",
    "    # Finding the answer and links to table\n",
    "    qs = []\n",
    "    ans = []\n",
    "    links = []\n",
    "    \n",
    "    # Mapping entity link to cell, entity link to surface word\n",
    "    #mapping_entity_loc = {}\n",
    "    mapping_entity = {}\n",
    "    for row_idx, row in enumerate(table['data']):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            for i, ent in enumerate(cell[1]):\n",
    "                if ent:\n",
    "                    if ent not in mapping_entity:\n",
    "                        mapping_entity[ent] = {cell[0][i]: [(row_idx, col_idx)]}\n",
    "                    else:\n",
    "                        if cell[0][i] not in mapping_entity[ent]:\n",
    "                            mapping_entity[ent][cell[0][i]] = [(row_idx, col_idx)]\n",
    "                        else:\n",
    "                            mapping_entity[ent][cell[0][i]] = mapping_entity[ent][cell[0][i]] + [(row_idx, col_idx)]\n",
    "    \n",
    "    for col_idx, header in enumerate(table['header']):\n",
    "        for i, ent in enumerate(header[1]):\n",
    "            if ent:\n",
    "                if ent not in mapping_entity:\n",
    "                    mapping_entity[ent] = {header[0][i]: [(-1, col_idx)]}\n",
    "                else:\n",
    "                    if header[0][i] not in mapping_entity[ent]:\n",
    "                        mapping_entity[ent][header[0][i]] = [(-1, col_idx)]\n",
    "                    else:\n",
    "                        mapping_entity[ent][header[0][i]] = mapping_entity[ent][header[0][i]] + [(-1, col_idx)]\n",
    "    \n",
    "    debug = False\n",
    "    # loop through the qa pairs\n",
    "    for q, a in d[1:]:\n",
    "        correction, tmp = searchForAnswer(a, table, requested_documents, mapping_entity)\n",
    "        if len(tmp) == 0 and len(a) >= 3 and not a.isdigit():\n",
    "            # See if the space becomes a problem\n",
    "            correction, tmp = searchForAnswerWithoutSpace(a, requested_documents, mapping_entity)\n",
    "            if len(tmp) > 0:\n",
    "                if debug:\n",
    "                    print(\"correct span! {} -> {}\".format(a, correction))\n",
    "                pass\n",
    "            else:\n",
    "                # correct the spelling                \n",
    "                tmp = get_edit_distance_equal_1(a, table)\n",
    "                if len(tmp) > 0:\n",
    "                    if debug:\n",
    "                        print(\"correct spelling! {} -> {}\".format(a, tmp[0][0]))\n",
    "                    correction = tmp[0][0]\n",
    "                else:\n",
    "                    # Split the answer\n",
    "                    fixed_as = fixing_answer(a)\n",
    "                    if fixed_as:\n",
    "                        for correction in fixed_as:\n",
    "                            _, tmp = searchForAnswer(correction, table, requested_documents, mapping_entity)\n",
    "                            if len(tmp) > 0:\n",
    "                                if debug:\n",
    "                                    print(\"correct splitting! {} -> {}\".format(a, correction))\n",
    "                                break\n",
    "            \n",
    "            if len(tmp) > 4:\n",
    "                if debug:\n",
    "                    print(\"many uncertainties for {}, decide not to replace it\".format(a))\n",
    "                tmp = []\n",
    "            elif correction and len(tmp) > 0:\n",
    "                a = correction.strip()\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        elif correction:\n",
    "            if len(correction) < 2 * len(a) : \n",
    "                if debug:\n",
    "                    print(\"cell correction! {} -> {}\".format(a, correction))\n",
    "                a = correction.strip()\n",
    "        \n",
    "        ans.append((a, tmp))\n",
    "        qs.append(q)\n",
    "        \n",
    "\n",
    "    keys = []\n",
    "    paras = []\n",
    "    for k, v in requested_documents.items():\n",
    "        for _ in tokenizer.tokenize(v):\n",
    "            keys.append(k)\n",
    "            paras.append(_)\n",
    "    \n",
    "    try:\n",
    "        para_feature = tfidf.fit_transform(paras)\n",
    "        q_feature = tfidf.transform(qs)\n",
    "    except Exception:\n",
    "        print(\"failed on table {}\".format(table_id))\n",
    "        return []\n",
    "        \n",
    "    dist_match = longest_match_distance(qs, paras)\n",
    "    dist = pairwise_distances(q_feature, para_feature, 'cosine')\n",
    "    \n",
    "    threshold = 0.95\n",
    "    best_threshold = 0.70\n",
    "    for i in range(len(qs)):\n",
    "        tfidf_nodes = []\n",
    "        string_nodes = []\n",
    "\n",
    "        min_dist = {}\n",
    "        tfidf_best_match = ('N/A', None, 1.)\n",
    "        for k, para, d in zip(keys, paras, dist[i]):\n",
    "            if d < min_dist.get(k, threshold):\n",
    "                min_dist[k] = d\n",
    "                if d < tfidf_best_match[-1]:\n",
    "                    tfidf_best_match = (k, para, d)\n",
    "                if d <= best_threshold:\n",
    "                    for content, locs in mapping_entity[k].items():\n",
    "                        for loc in locs:\n",
    "                            tfidf_nodes.append((content, loc, k, para, d))\n",
    "        \n",
    "        if tfidf_best_match[0] != 'N/A':\n",
    "            if tfidf_best_match[-1] > best_threshold:\n",
    "                for content, locs in mapping_entity[k].items():\n",
    "                    for loc in locs:\n",
    "                        tfidf_nodes.append((content, loc, k, tfidf_best_match[1], tfidf_best_match[2]))\n",
    "\n",
    "        min_dist = {}\n",
    "        string_best_match = ('N/A', None, 1.)\n",
    "        for k, para, d in zip(keys, paras, dist_match[i]):\n",
    "            if d < min_dist.get(k, threshold):\n",
    "                min_dist[k] = d\n",
    "                if d < string_best_match[-1]:\n",
    "                    string_best_match = (k, para, d)\n",
    "                if d <= best_threshold:\n",
    "                    for content, locs in mapping_entity[k].items():\n",
    "                        for loc in locs:\n",
    "                            string_nodes.append((content, loc, k, para, d))\n",
    "                    \n",
    "        if string_best_match[0] != 'N/A':\n",
    "            if string_best_match[-1] > best_threshold:\n",
    "                for content, locs in mapping_entity[k].items():\n",
    "                    for loc in locs:\n",
    "                        string_nodes.append((content, loc, k, string_best_match[1], string_best_match[2]))\n",
    "        \n",
    "        results.append({'table_id': table_id, 'question': qs[i], 'answer-text': ans[i][0], \n",
    "                        'answer-node': ans[i][1], 'tf-idf': tfidf_nodes, \n",
    "                        'string-overlap': string_nodes})\n",
    "    \n",
    "    return results\n",
    "\n",
    "def convert2num(string):\n",
    "    string = string.replace(',', '')\n",
    "    if string.endswith('%'):\n",
    "        string = string.rstrip('%')\n",
    "    try:\n",
    "        string = float(string)\n",
    "        return string\n",
    "    except Exception:\n",
    "        return None\n",
    "    \n",
    "def find_superlative(table_id, table):\n",
    "    if not os.path.exists('tables_tmp/{}.json'.format(table_id)):\n",
    "        mapping = {}\n",
    "        headers = [_[0][0] for _ in table['header']]\n",
    "        for j in range(len(table['header'])):\n",
    "            mapping[headers[j]] = []\n",
    "            activate_date_or_num = None\n",
    "            if headers[j] not in ['#', 'Type', 'Name', 'Location', 'Position', 'Category', 'Nationality',\n",
    "                                  'School', 'Notes', 'Notability', 'Country']:\n",
    "                for i, row in enumerate(table['data']):\n",
    "                    if len(table['data'][i][j][0]) > 1:\n",
    "                        continue\n",
    "\n",
    "                    data = table['data'][i][j][0][0]\n",
    "                    if data in ['', '-']:\n",
    "                        continue\n",
    "\n",
    "                    num = convert2num(data)\n",
    "                    if num and data.isdigit() and num > 1000 and num < 2020 and activate_date_or_num in ['date', None]:\n",
    "                        date_format = parse(data)\n",
    "                        mapping[headers[j]].append((date_format, 'date', [data, (i, j), None, None, 1.0]))\n",
    "                        activate_date_or_num = 'date'\n",
    "                    elif num and activate_date_or_num in ['num', None]: \n",
    "                        mapping[headers[j]].append((num, 'number', [data, (i, j), None, None, 1.0]))\n",
    "                        activate_date_or_num = 'num'\n",
    "                    else:\n",
    "                        try:\n",
    "                            date_format = parse(data)\n",
    "                            if date_format and activate_date_or_num in ['date', None]:\n",
    "                                mapping[headers[j]].append((date_format, 'date', [data, (i, j), None, None, 1.0]))\n",
    "                                activate_date_or_num = 'date'\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "            if len(mapping[headers[j]]) < 0.3 * len(table['data']):\n",
    "                mapping[headers[j]] = []\n",
    "\n",
    "        nodes = []\n",
    "        for k, v in mapping.items():\n",
    "            if len(v) > 0:\n",
    "                tmp = sorted(v, key = lambda x: x[0])\n",
    "                if tmp[0][1] == 'number':\n",
    "                    tmp_node = tmp[0][-1]\n",
    "                    tmp_node[3] = 'minimum'\n",
    "                    nodes.append(tmp_node)\n",
    "                    tmp_node = tmp[-1][-1]\n",
    "                    tmp_node[3] = 'maximum'\n",
    "                    nodes.append(tmp_node)\n",
    "                else:\n",
    "                    tmp_node = tmp[0][-1]\n",
    "                    tmp_node[3] = 'earliest'\n",
    "                    nodes.append(tmp_node)\n",
    "                    tmp_node = tmp[-1][-1]\n",
    "                    tmp_node[3] = 'latest'\n",
    "                    nodes.append(tmp_node)\n",
    "\n",
    "        with open('tables_tmp/{}.json'.format(table_id), 'w') as f:\n",
    "            json.dump(nodes, f)\n",
    "    else:\n",
    "        with open('tables_tmp/{}.json'.format(table_id), 'r') as f:\n",
    "            nodes = json.load(f)\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def step2(d):\n",
    "    threshold = 90\n",
    "    # LINKING THE CELL DATA\n",
    "    triggers = ['JJR', 'JJS', 'RBR', 'RBS']\n",
    "    \n",
    "    new_processed = []\n",
    "\n",
    "    table_id = d['table_id']\n",
    "    with open('tables_tok/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)        \n",
    "\n",
    "    tmp_link = []\n",
    "    for row_idx, row in enumerate(table['data']):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            if cell[0] != ['']:\n",
    "                for ent in cell[0]:\n",
    "                    ratio = fuzz.partial_ratio(' ' + ent.lower() + ' ', ' ' + d['question'].lower() + ' ')\n",
    "                    if ratio > threshold:\n",
    "                        tmp_link.append((ent, (row_idx, col_idx), None, 'string match', ratio / 100))\n",
    "\n",
    "    pos_tags = \" \".join([_[1] for _ in nltk.pos_tag(d['question'].split(' '))])\n",
    "    d['question_postag'] = pos_tags\n",
    "\n",
    "    d['links'] = tmp_link\n",
    "    if any([_ in pos_tags for _ in triggers]):\n",
    "        #try:\n",
    "        tmp = find_superlative(table_id, table)\n",
    "        d['links'] = d['links'] + tmp\n",
    "        #except Exception:\n",
    "        #    print(\"failed with table {}\".format(table_id))\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def hash_string(string):\n",
    "    import hashlib\n",
    "    sha = hashlib.sha256()\n",
    "    sha.update(string.encode())\n",
    "    return sha.hexdigest()[:16]\n",
    "\n",
    "\n",
    "def step3(processed):\n",
    "    trivial, easy, medium, hard, no_answer, number, yesorno, repeated = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    from_passage, from_cell, from_calculation = 0, 0, 0\n",
    "    new_processed = []\n",
    "    used_question_id = set([])\n",
    "    \n",
    "    question_type = ''\n",
    "    where_from = ''\n",
    "    for p in processed:\n",
    "        p['question_id'] = hash_string(p['question'])\n",
    "        \n",
    "        if p['question_id'] in used_question_id:\n",
    "            repeated +=1\n",
    "            continue\n",
    "        elif p['answer-text'].lower() in ['yes', 'no']:\n",
    "            yesorno += 1\n",
    "            continue\n",
    "        elif len(p['answer-text'].split(' ')) > 15:\n",
    "            yesorno += 1\n",
    "            continue\n",
    "        else:\n",
    "            if len(p['answer-node']) > 1 and p['answer-node'][0][-1] == 'table':\n",
    "                with open('tables_tok/{}.json'.format(p['table_id']), 'r') as f:\n",
    "                    table = json.load(f)\n",
    "                headers = [\" , \".join(cell[0]) for cell in table['header']]\n",
    "                potential_headers = set()\n",
    "                for h in headers:\n",
    "                    if \" \" + h.lower() + \" \" in \" \" + p['question'].lower() + \" \":\n",
    "                        potential_headers.add(h)\n",
    "\n",
    "                if len(potential_headers) > 0:\n",
    "                    remaining_nodes = []\n",
    "                    for n in p['answer-node']:\n",
    "                        if headers[n[1][1]] in potential_headers:\n",
    "                            remaining_nodes.append(n)\n",
    "\n",
    "                    if len(remaining_nodes) > 0 and len(remaining_nodes) < len(p['answer-node']):\n",
    "                        p['answer-node'] = remaining_nodes\n",
    "\n",
    "            # Categorize the difficulty level\n",
    "            number_trigger = ['how many', 'how much', 'how long', 'how far', 'how old', 'difference', 'total']\n",
    "            answer_node = p['answer-node']\n",
    "            if len(answer_node) == 0:\n",
    "                if any([x in p['question'].lower() for x in number_trigger]):\n",
    "                    number += 1\n",
    "                    question_type = 'numeric'\n",
    "                else:\n",
    "                    no_answer += 1\n",
    "                    continue\n",
    "            else:\n",
    "                matching_cells = []\n",
    "                if p['tf-idf']:\n",
    "                    matching_cells.extend([tuple(_[1]) for _ in p['tf-idf']])\n",
    "                if p['string-overlap']:\n",
    "                    matching_cells.extend([tuple(_[1]) for _ in p['string-overlap']])\n",
    "                linking_cells = [tuple(_[1]) for _ in p['links']]\n",
    "                matching_cells = set(matching_cells)\n",
    "                linking_cells = set(linking_cells)\n",
    "\n",
    "                evidence_cells = matching_cells | linking_cells\n",
    "                answer_row = set([_[1][0] for _ in answer_node])\n",
    "                evidence_row = set([_[0] for _ in evidence_cells])\n",
    "                intersect_row = answer_row & evidence_row\n",
    "\n",
    "                if len(intersect_row) > 0:\n",
    "                    new_answer_nodes = []\n",
    "                    for node in p['answer-node']:\n",
    "                        if node[1][0] in intersect_row:\n",
    "                            new_answer_nodes.append(node)\n",
    "                    p['answer-node'] = new_answer_nodes\n",
    "                \n",
    "                answer_cells = set([tuple(_[1]) for _ in p['answer-node']])\n",
    "                if len(evidence_cells & answer_cells) > 0:\n",
    "                    new_answer_nodes = []\n",
    "                    for node in p['answer-node']:\n",
    "                        if tuple(node[1]) in evidence_cells:\n",
    "                            new_answer_nodes.append(node)\n",
    "                    p['answer-node'] = new_answer_nodes\n",
    "                \n",
    "                answer_cells = set([tuple(_[1]) for _ in p['answer-node']])\n",
    "                \n",
    "                if matching_cells == answer_cells:\n",
    "                    trivial += 1\n",
    "                    question_type = 'trivial'\n",
    "                elif len(evidence_cells & answer_cells) > 0:\n",
    "                    easy += 1\n",
    "                    question_type = 'easy'\n",
    "                else:\n",
    "                    if len(intersect_row) > 0:\n",
    "                        medium += 1\n",
    "                        question_type = 'medium'\n",
    "                    else:\n",
    "                        hard += 1\n",
    "                        question_type = 'hard'\n",
    "\n",
    "        p['type'] = question_type\n",
    "\n",
    "        if p['type'] != 'trivial':\n",
    "            if len(answer_node) > 0:\n",
    "                if answer_node[0][-1] == 'passage':\n",
    "                    from_passage += 1\n",
    "                else:\n",
    "                    from_cell += 1\n",
    "                p['where'] = answer_node[0][-1]\n",
    "            else:\n",
    "                from_calculation += 1\n",
    "                p['where'] = 'calculation'\n",
    "                \n",
    "            new_processed.append(p)\n",
    "            used_question_id.add(p['question_id'])\n",
    "\n",
    "    print(\"trivial: {}, easy: {}, medium: {}, hard: {}, number: {}, no answer: {}, yes/no: {}, repeated: {}\".\n",
    "          format(trivial, easy, medium, hard, number, no_answer, yesorno, repeated))\n",
    "    print(\"from cell: {}, from passage: {}, from_calculation: {}\".format(from_cell, from_passage, from_calculation))\n",
    "\n",
    "    return new_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/collected_data.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"running data for {} tables\".format(len(data)))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(64)\n",
    "results_func = pool.map(step1, data)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = []\n",
    "for _ in results_func:\n",
    "    results.extend(_)\n",
    "\n",
    "#with open('Mixed-Reasoning/processed_step1.json', 'w') as f:\n",
    "#    json.dump(results, f, indent=2)\n",
    "    \n",
    "#with open('Mixed-Reasoning/processed_step1.json', 'r') as f:\n",
    "#    processed = json.load(f)\n",
    "\n",
    "pool = Pool(64)\n",
    "    \n",
    "results = pool.map(step2, results)\n",
    "\n",
    "pool.close()\n",
    "pool.join()    \n",
    "#with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "#    processed = json.load(f)\n",
    "with open('Mixed-Reasoning/processed_step2.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/collected_data.json') as f:\n",
    "    data = json.load(f)\n",
    "print(\"running data for {} tables\".format(len(data)))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(64)\n",
    "results_func = pool.map(step1, data)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = []\n",
    "for _ in results_func:\n",
    "    results.extend(_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"before filtering, there are {} instances\".format(len(results)))\n",
    "processed = step3(results)\n",
    "processed = sorted(processed, key=lambda x: x['question_id'])\n",
    "\n",
    "#with open('Mixed-Reasoning/processed_step3_indent2.json', 'w') as f:\n",
    "#    json.dump(processed, f, indent=2)\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'w') as f:\n",
    "    json.dump(processed, f)\n",
    "\n",
    "num_slots = 0\n",
    "for d in processed:\n",
    "    num_slots += len(d['tf-idf']) + len(d['string-overlap']) + len(d['links'])\n",
    "\n",
    "print(\"average link is {} for {} instances\".format(num_slots / len(processed), len(processed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    new_processed = json.load(f)\n",
    "\n",
    "with open('Mixed-Reasoning/dev_ids.json') as f:\n",
    "    dev_ids = json.load(f)\n",
    "\n",
    "medium_split = []\n",
    "for d in new_processed:\n",
    "    if d['type'] == 'medium':\n",
    "        tmp = set()\n",
    "        for n in d['answer-node']:\n",
    "            tmp.add((n[0], n[2]))\n",
    "        if len(tmp) == 1:\n",
    "            #with open('request_tok/{}.json'.format(d['table_id'])) as f:\n",
    "            #    request_document = json.load(f)\n",
    "            #document = request_document[list(tmp)[0][1]]\n",
    "            #if len(document.split(' ')) > 100:\n",
    "            if d['question_id'] in dev_ids:\n",
    "                medium_split.append({'table_id': d['table_id'], 'question_id': d['question_id'],\n",
    "                                    'answer-text': d['answer-text'], 'answer-node': d['answer-node'][0],\n",
    "                                    'question': d['question']})\n",
    "            #else:\n",
    "            #    pass\n",
    "print(\"The medium split has {} entries\".format(len(medium_split)))\n",
    "\n",
    "with open('Mixed-Reasoning/dev_processed_step3_medium.json', 'w') as f:\n",
    "    json.dump(medium_split, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import *\n",
    "import math\n",
    "import seaborn as sns\n",
    "import copy\n",
    "\n",
    "n = numpy.zeros((20, 6))\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "for d in data:\n",
    "    table_id = d['table_id']\n",
    "    with open('tables/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)\n",
    "    columns = len(table['header'])\n",
    "    rows = len(table['data'])\n",
    "    \n",
    "    for node in d['answer-node']:\n",
    "        i = round((node[1][0] + 1) / rows * 20) - 1\n",
    "        j = round((node[1][1] + 1) / columns * 6) - 1\n",
    "        if n[i, j] < 1200:\n",
    "            n[i, j] += 1\n",
    "\n",
    "new_n = copy.copy(n)\n",
    "for i in range(0, 5):\n",
    "    new_n[:, i] = (new_n[:, i] + new_n[:, i + 1]) / 2\n",
    "    \n",
    "sns.heatmap(new_n, cmap=\"YlGnBu\", xticklabels=False, yticklabels=False)\n",
    "plt.savefig('table-distribution.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/stage3_training_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "total = 484\n",
    "grid_size = int(math.sqrt(total))\n",
    "\n",
    "counter = Counter()\n",
    "for d in data:\n",
    "    start_position = d['answers'][0]['answer_start']\n",
    "    if start_position != 8:\n",
    "        counter.update([start_position])\n",
    "        \n",
    "heat = [0 for _ in range(0, total)]\n",
    "for c, n in counter.most_common():\n",
    "    if c < total:\n",
    "        heat[c] = min(n, 100)\n",
    "\n",
    "n = numpy.array(heat).reshape([grid_size, grid_size])\n",
    "\n",
    "sns.heatmap(n, cmap=\"YlGnBu\", xticklabels=False, yticklabels=False)\n",
    "plt.savefig('passage-distribution.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fn in os.listdir('tables/'):\n",
    "    if fn.endswith('.json'):\n",
    "        with open('tables/{}'.format(fn)) as f:\n",
    "            table = json.load(f)\n",
    "        \n",
    "        headers = table['header']\n",
    "        if headers[0][0] == ['']:\n",
    "            for i in range(len(table['data'])):\n",
    "                del table['data'][i][0]\n",
    "        \n",
    "            del headers[0]\n",
    "\n",
    "            with open('tables/{}'.format(fn), 'w') as f:\n",
    "                json.dump(table, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in os.listdir('tables/'):\n",
    "    if fn.endswith('.json'):\n",
    "        with open('tables/{}'.format(fn)) as f:\n",
    "            table = json.load(f)\n",
    "\n",
    "        headers = table['header']\n",
    "\n",
    "        if any([_[0] == ['Rank'] for _ in headers]):\n",
    "            if table['data'][0][0][0] == ['']:\n",
    "                for i in range(len(table['data'])):\n",
    "                    if table['data'][i][0][0] == ['']:\n",
    "                        table['data'][i][0][0] = [str(i + 1)]\n",
    "                \n",
    "                with open('tables/{}'.format(fn), 'w') as f:\n",
    "                    json.dump(table, f, indent=2)\n",
    "        \n",
    "        if any([_[0] == ['Place'] for _ in headers]):\n",
    "            if table['data'][0][0][0] == ['']:\n",
    "                for i in range(len(table['data'])):\n",
    "                    if table['data'][i][0][0] == ['']:\n",
    "                        table['data'][i][0][0] = [str(i + 1)]\n",
    "                \n",
    "                with open('tables/{}'.format(fn), 'w') as f:\n",
    "                    json.dump(table, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_cell_text(string):\n",
    "    string = string.rstrip('^')\n",
    "    string = string.replace('\\u200e', '')\n",
    "    string = string.replace('\\ufeff', '')\n",
    "    string = string.replace('â€“', '-')\n",
    "    string = string.replace(u'\\u2009', u' ')\n",
    "    string = string.replace(u'\\u2010', u' - ')\n",
    "    string = string.replace(u'\\u2011', u' - ')\n",
    "    string = string.replace(u'\\u2012', u' - ')\n",
    "    string = string.replace(u'\\u2013', u' - ')\n",
    "    string = string.replace(u'\\u2014', u' - ')\n",
    "    string = string.replace(u'\\u2015', u' - ')\n",
    "    string = string.replace(u'\\u2018', u'')\n",
    "    string = string.replace(u'\\u2019', u'')\n",
    "    string = string.replace(u'\\u201c', u'')\n",
    "    string = string.replace(u'\\u201d', u'')\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = string.strip()\n",
    "    return string\n",
    "    \n",
    "for fn in os.listdir('tables/'):\n",
    "    with open('tables/{}'.format(fn)) as f:\n",
    "        table = json.load(f)\n",
    "    \n",
    "    for row_idx, row in enumerate(table['data']):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            for i, ent in enumerate(cell[0]):\n",
    "                if ent:\n",
    "                    table['data'][row_idx][col_idx][0][i] = clean_cell_text(ent)\n",
    "    \n",
    "    for col_idx, header in enumerate(table['header']):\n",
    "        for i, ent in enumerate(header[0]):\n",
    "            if ent:\n",
    "                table['header'][col_idx][0][i] = clean_cell_text(ent)\n",
    "    \n",
    "    with open('tables/{}'.format(fn), 'w') as f:\n",
    "        json.dump(table, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from utils import url2dockey, filter_firstKsents\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#train_keys = []\n",
    "#dev_keys = []\n",
    "medium_keys = []\n",
    "hard_keys = []\n",
    "for k in data:\n",
    "    if k['type'] in ['medium', 'numeric']:\n",
    "        medium_keys.append(k['question_id'])\n",
    "    elif k['type'] in ['hard']:\n",
    "        hard_keys.append(k['question_id'])\n",
    "\n",
    "random.shuffle(medium_keys)\n",
    "random.shuffle(hard_keys)\n",
    "\n",
    "size = 8000\n",
    "dev_keys = medium_keys[:size] + hard_keys[:1000]\n",
    "test_keys = medium_keys[size:size * 2] + hard_keys[1000:2000]\n",
    "\n",
    "\n",
    "with open('Mixed-Reasoning/dev_ids.json', 'w') as f:\n",
    "    json.dump(dev_keys, f)\n",
    "    \n",
    "with open('Mixed-Reasoning/test_ids.json', 'w') as f:\n",
    "    json.dump(test_keys, f)\n",
    "\n",
    "with open('Mixed-Reasoning/medium_dev_ids.json', 'w') as f:\n",
    "    json.dump(medium_keys[:size], f)\n",
    "\n",
    "with open('Mixed-Reasoning/hard_dev_ids.json', 'w') as f:\n",
    "    json.dump(hard_keys[:1000], f)\n",
    "\n",
    "\"\"\"\n",
    "dev_size = int(len(data) * 0.1)\n",
    "random.shuffle(potential_dev_keys)\n",
    "\n",
    "dev_keys = potential_dev_keys[:int(dev_size//2)]\n",
    "test_keys = potential_dev_keys[int(dev_size//2) : dev_size]\n",
    "\n",
    "train_keys = train_keys + potential_dev_keys[dev_size:]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for opt in ['dev', 'test']:\n",
    "    with open('Mixed-Reasoning/code/predictions.{}.json'.format(opt), 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    wrong_question_ids = []\n",
    "    correct_question_ids = []\n",
    "    for r in results:\n",
    "        if r['answer-text'] != r['pred']:\n",
    "            wrong_question_ids.append(r['question_id'])\n",
    "        else:\n",
    "            correct_question_ids.append(r['question_id'])\n",
    "\n",
    "    random.shuffle(wrong_question_ids)\n",
    "    random.shuffle(correct_question_ids)\n",
    "\n",
    "    wrong_question_ids = wrong_question_ids[:2000]\n",
    "    correct_question_ids = correct_question_ids[:1500]\n",
    "\n",
    "    dev_ids = wrong_question_ids + correct_question_ids\n",
    "    random.shuffle(dev_ids)\n",
    "\n",
    "    with open('Mixed-Reasoning/filtered_{}_ids.json'.format(opt), 'w') as f:\n",
    "        json.dump(dev_ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the released dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "with open('Mixed-Reasoning/filtered_dev_ids.json', 'r') as f:\n",
    "    dev_keys = set(json.load(f))\n",
    "with open('Mixed-Reasoning/filtered_test_ids.json', 'r') as f:\n",
    "    test_keys = set(json.load(f))\n",
    "\n",
    "if not os.path.exists('Mixed-Reasoning/released_data'):\n",
    "    os.mkdir('Mixed-Reasoning/released_data')\n",
    "\n",
    "train_data = []\n",
    "dev_data = []\n",
    "test_data = []\n",
    "\n",
    "dev_reference = {'reference': {}, 'passage':[], 'table': []}\n",
    "test_reference = {'reference': {}, 'passage':[], 'table': []}\n",
    "\n",
    "for d in data:\n",
    "    if d['question_id'] in dev_keys:\n",
    "        del d['tf-idf']\n",
    "        del d['string-overlap']\n",
    "        del d['links']\n",
    "        del d['type']\n",
    "        dev_data.append(d)\n",
    "        dev_reference['reference'][d['question_id']] = d['answer-text']\n",
    "        if d['where'] == 'table':\n",
    "            dev_reference['table'].append(d['question_id'])\n",
    "        else:\n",
    "            dev_reference['passage'].append(d['question_id'])\n",
    "        \n",
    "    elif d['question_id'] in test_keys:\n",
    "        del d['tf-idf']\n",
    "        del d['string-overlap']\n",
    "        del d['links']\n",
    "        del d['type']\n",
    "        test_data.append(d)\n",
    "        test_reference['reference'][d['question_id']] = d['answer-text']\n",
    "        if d['where'] == 'table':\n",
    "            test_reference['table'].append(d['question_id'])\n",
    "        else:\n",
    "            test_reference['passage'].append(d['question_id'])\n",
    "        del d['answer-text']\n",
    "        del d['answer-node']\n",
    "        del d['where']    \n",
    "    else:\n",
    "        del d['tf-idf']\n",
    "        del d['string-overlap']\n",
    "        del d['links']\n",
    "        del d['type']\n",
    "        train_data.append(d)\n",
    "\n",
    "with open('Mixed-Reasoning/released_data/train.json', 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/released_data/dev.json', 'w') as f:\n",
    "    json.dump(dev_data, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/released_data/test.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/released_data/dev_reference.json', 'w') as f:\n",
    "    json.dump(dev_reference, f)\n",
    "\n",
    "with open('Mixed-Reasoning/released_data/test_reference.json', 'w') as f:\n",
    "    json.dump(test_reference, f)\n",
    "    \n",
    "print(\"there are {} training, {} dev, {} test instances\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Test/Dev Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/dev_ids.json', 'r') as f:\n",
    "    dev_keys = set(json.load(f))\n",
    "with open('Mixed-Reasoning/test_ids.json', 'r') as f:\n",
    "    test_keys = set(json.load(f))\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "dev_split = []\n",
    "\n",
    "for d in data:\n",
    "    if d['question_id'] in dev_keys:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        headers = [cell[0][0] for cell in table['header']]\n",
    "\n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = set([_[1][0] for _ in answer_nodes])\n",
    "\n",
    "        tmp = []\n",
    "        labels = []\n",
    "        for node in d['tf-idf']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'tf-idf'])\n",
    "        for node in d['string-overlap']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'string-overlap'])            \n",
    "        for node in d['links']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'links'])\n",
    "        dev_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                          'nodes': tmp, 'answer-text': d['answer-text']})\n",
    "        \n",
    "with open('Mixed-Reasoning/dev_input.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "    \n",
    "test_split = []\n",
    "\n",
    "for d in data:\n",
    "    if d['question_id'] in test_keys:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        headers = [cell[0][0] for cell in table['header']]\n",
    "\n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = set([_[1][0] for _ in answer_nodes])\n",
    "\n",
    "        tmp = []\n",
    "        labels = []\n",
    "        for node in d['tf-idf']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'tf-idf'])\n",
    "        for node in d['string-overlap']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'string-overlap'])            \n",
    "        for node in d['links']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'links'])\n",
    "        \n",
    "        test_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                          'nodes': tmp, 'answer-text': d['answer-text']})\n",
    "        \n",
    "with open('Mixed-Reasoning/test_input.json', 'w') as f:\n",
    "    json.dump(test_split, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the stage1/2/3 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/dev_ids.json', 'r') as f:\n",
    "    dev_keys = json.load(f)\n",
    "    \n",
    "with open('Mixed-Reasoning/test_ids.json', 'r') as f:\n",
    "    test_keys = json.load(f)\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "with open('Mixed-Reasoning/dev_ids.json', 'r') as f:\n",
    "    dev_keys = set(json.load(f))\n",
    "\n",
    "train_split = []\n",
    "dev_split = []\n",
    "\n",
    "for d in data:\n",
    "    if d['type'] in ['medium', 'easy']:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        headers = [\" , \".join(cell[0]) for cell in table['header']]\n",
    "\n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = set([_[1][0] for _ in answer_nodes])\n",
    "\n",
    "        tmp = []\n",
    "        labels = []\n",
    "        for node in d['tf-idf']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'tf-idf'])        \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        for node in d['string-overlap']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'string-overlap'])        \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        for node in d['links']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'links'])   \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        \n",
    "        if d['question_id'] not in dev_keys and d['question_id'] not in test_keys:\n",
    "            train_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                              'nodes': tmp, 'labels': labels})\n",
    "        elif d['question_id'] in dev_keys:\n",
    "            dev_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                              'nodes': tmp, 'labels': labels})\n",
    "\n",
    "with open('Mixed-Reasoning/stage1_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage1_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "    \n",
    "print(\"Done with Stage1 Data Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def func(d):\n",
    "    train_split, dev_split = [], []\n",
    "    if d['type'] in ['medium', 'easy']:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            requested_document = json.load(f)\n",
    "        \n",
    "        headers = [cell[0][0] for cell in table['header']]\n",
    "        \n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = {_[1][0]: _ for _ in answer_nodes}\n",
    "\n",
    "        labels = []\n",
    "        for name, source in zip(['tf-idf', 'string-overlap', 'links'], [d['tf-idf'], d['string-overlap'], d['links']]):\n",
    "            for node in source:\n",
    "                i = node[1][0]\n",
    "                if i in answer_rows and i >= 0:\n",
    "                    tmp = {'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], 'current': node + [headers[node[1][1]], name]}\n",
    "                    target_nodes = []\n",
    "                    labels = []\n",
    "                    same_row = table['data'][i]\n",
    "                    for j, cell in enumerate(same_row):\n",
    "                        for content, url in zip(cell[0], cell[1]):\n",
    "                            if len(content) > 0:\n",
    "                                if url:\n",
    "                                    doc = requested_document[url]\n",
    "                                    intro = filter_firstKsents(doc, 1)\n",
    "                                    target_nodes.append((content, (i, j), url, headers[j], intro))\n",
    "                                    if url == answer_rows[i][2]:\n",
    "                                        labels.append(1)\n",
    "                                    else:\n",
    "                                        labels.append(0)\n",
    "                                else:\n",
    "                                    target_nodes.append((content, (i, j), None, headers[j], ''))\n",
    "                                    if content == answer_rows[i][0]:\n",
    "                                        labels.append(1)\n",
    "                                    else:\n",
    "                                        labels.append(0)\n",
    "                                \n",
    "                        if len(cell[0]) > 1:\n",
    "                            content = ' , '.join(cell[0])\n",
    "                            if content == answer_rows[i][0]:\n",
    "                                labels.append(1)\n",
    "                            else:\n",
    "                                labels.append(0)\n",
    "                                \n",
    "                            target_nodes.append((content, (i, j), None, headers[j], ''))\n",
    "                        \n",
    "                    tmp['labels'] = labels\n",
    "\n",
    "                    assert sum(labels) > 0, d['question_id']\n",
    "                    \n",
    "                    tmp['target'] = target_nodes\n",
    "\n",
    "                    if tmp['question_id'] not in dev_keys and tmp['question_id'] not in test_keys:\n",
    "                        train_split.append(tmp)\n",
    "                    elif tmp['question_id'] in dev_keys:\n",
    "                        dev_split.append(tmp)\n",
    "\n",
    "    return train_split, dev_split\n",
    "\n",
    "\n",
    "pool = Pool(64)\n",
    "results = pool.map(func, data)\n",
    "\n",
    "train_split = []\n",
    "dev_split = []\n",
    "for r1, r2 in results:\n",
    "    train_split.extend(r1)\n",
    "    dev_split.extend(r2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage2_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage2_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "\n",
    "print(\"Done with Stage2 Data Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = []\n",
    "dev_split = []\n",
    "inside, self = 0, 0\n",
    "for d in data:\n",
    "    if d['where'] == 'passage':\n",
    "        table_id = d['table_id']\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "            requested_documents = json.load(f)        \n",
    "        \n",
    "        #tmp = mapping.get(str(table_id), [])\n",
    "        \n",
    "        used = set()\n",
    "        for node in d['answer-node']:\n",
    "            if node[2] not in used:\n",
    "                context = requested_documents[node[2]]\n",
    "                context = 'Title : {} . '.format(node[0]) + context\n",
    "                \n",
    "                orig_answer = d['answer-text']\n",
    "\n",
    "                start = context.lower().find(orig_answer.lower())\n",
    "\n",
    "                if start == -1:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                while context[start].lower() != orig_answer[0].lower():\n",
    "                    start -= 1\n",
    "\n",
    "                answer = context[start:start+len(orig_answer)]\n",
    "                #assert(answer.lower() == orig_answer.lower(), \"{} -> {}\".format(answer, orig_answer))\n",
    "                \n",
    "                if d['question_id'] not in dev_keys and d['question_id'] not in test_keys:\n",
    "                    train_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                elif d['question_id'] in dev_keys:\n",
    "                    dev_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                inside += 1\n",
    "                used.add(node[2])\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    if d['where'] == 'table':\n",
    "        table_id = d['table_id']\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "            requested_documents = json.load(f)  \n",
    "            \n",
    "        used = set()\n",
    "        for node in d['answer-node']:\n",
    "            if node[2] and node[2] not in used:\n",
    "                context = requested_documents[node[2]]\n",
    "                context = 'Title : {} . '.format(node[0]) + context\n",
    "                \n",
    "                orig_answer = node[0]\n",
    "\n",
    "                start = context.lower().find(orig_answer.lower())\n",
    "\n",
    "                if start == -1:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                while context[start].lower() != orig_answer[0].lower():\n",
    "                    start -= 1\n",
    "                    \n",
    "                answer = context[start:start+len(orig_answer)]\n",
    "                \n",
    "                if d['question_id'] not in dev_keys and d['question_id'] not in test_keys:\n",
    "                    train_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                elif d['question_id'] in dev_keys:\n",
    "                    dev_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                self += 1\n",
    "                used.add(node[2])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "with open('Mixed-Reasoning/stage3_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "    \n",
    "with open('Mixed-Reasoning/stage3_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "\n",
    "#print(\"Total amount of training instance = {} and dev instance = {}\".format(len(training_data), len(dev_data)))\n",
    "print(\"Looking inside the passage = {} and self loop = {}\".format(inside, self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming all files into gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "fs = ['Mixed-Reasoning/stage1_training_data.json', 'Mixed-Reasoning/stage1_dev_data.json',\n",
    "    'Mixed-Reasoning/stage2_training_data.json', 'Mixed-Reasoning/stage2_dev_data.json', \n",
    "    'Mixed-Reasoning/stage3_training_data.json', 'Mixed-Reasoning/stage3_dev_data.json']\n",
    "\n",
    "for f_n in fs:\n",
    "    compressGZip(f_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/stage1_training_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "distribution = []\n",
    "for d in data:\n",
    "    distribution.append(len(d['nodes']))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n, bins, patches = plt.hist(distribution, 100, facecolor='blue', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating data for KBQA method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('Mixed-Reasoning/dev_ids.json', 'r') as f:\n",
    "    dev_ids = set(json.load(f))\n",
    "\n",
    "with open('Mixed-Reasoning/test_ids.json', 'r') as f:\n",
    "    test_ids = set(json.load(f))\n",
    "\n",
    "print(\"there are {} dev and {} testing instances\".format(len(dev_ids), len(test_ids)))\n",
    "def hash_string(string):\n",
    "    import hashlib\n",
    "    sha = hashlib.sha256()\n",
    "    sha.update(string.encode())\n",
    "    return sha.hexdigest()[:16]\n",
    "    \n",
    "new_data = []\n",
    "for p in data:\n",
    "    if p['answer-node'] and p['answer-node'][0][-1] == 'table':\n",
    "        if len(p['answer-node']) > 1 and p['answer-node'][0][-1] == 'table':\n",
    "            with open('tables_tok/{}.json'.format(p['table_id']), 'r') as f:\n",
    "                table = json.load(f)\n",
    "            headers = [\" , \".join(cell[0]) for cell in table['header']]    \n",
    "\n",
    "            potential_headers = set()\n",
    "            for h in headers:\n",
    "                if \" \" + h.lower() + \" \" in \" \" + p['question'].lower() + \" \":\n",
    "                    potential_headers.add(h)\n",
    "\n",
    "            if len(potential_headers) > 0:\n",
    "                remaining_nodes = []\n",
    "                for n in p['answer-node']:\n",
    "                    if headers[n[1][1]] in potential_headers:\n",
    "                        remaining_nodes.append(n)\n",
    "\n",
    "                if len(remaining_nodes) > 0 and len(remaining_nodes) < len(p['answer-node']):\n",
    "                    p['answer-node'] = remaining_nodes    \n",
    "        \n",
    "        del p['string-overlap']\n",
    "        del p['tf-idf']\n",
    "        \n",
    "        if len(p['links']) > 0:\n",
    "            new_data.append(p)\n",
    "\n",
    "print(\"totally {} instances\".format(len(new_data)))\n",
    "\n",
    "outputs = []\n",
    "for d in new_data:\n",
    "    question_id = hash_string(d['question'])\n",
    "    if question_id in dev_ids or question_id in test_ids:\n",
    "        #print(\"IN DEV/TEST\")\n",
    "        continue\n",
    "    \n",
    "    table_id = d['table_id']\n",
    "    with open('tables_tok/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)\n",
    "    headers = [_[0][0] for _ in table['header']]\n",
    "    \n",
    "    answer_mapping = {}\n",
    "    for node in d['answer-node']:\n",
    "        answer_mapping[node[1][0]] = [headers[node[1][1]]]\n",
    "    \n",
    "    for link in d['links']:\n",
    "        if link[1][0] in answer_mapping:\n",
    "            answer_mapping[node[1][0]].append(link)\n",
    "    \n",
    "    potential_link = []\n",
    "    for link in d['links']:\n",
    "        if (headers[link[1][1]], link[0]) not in potential_link:\n",
    "            if link[3] == 'string match':\n",
    "                potential_link.append((headers[link[1][1]], link[0]))\n",
    "    potential_link.append(('*', '*'))\n",
    "    \n",
    "    for k, v in answer_mapping.items():\n",
    "        if len(v) > 1:\n",
    "            tmp = {'aggregate': '', 'target': None, 'where': []}\n",
    "            \n",
    "            for link in v[1:]:\n",
    "                if link[3] != 'string match':\n",
    "                    tmp['aggregate'] = link[3]\n",
    "                    tmp['target'] = v[0]\n",
    "                else:\n",
    "                    tmp['target'] = v[0]\n",
    "                    if (headers[link[1][1]], link[0]) not in tmp['where']:\n",
    "                        tmp['where'].append((headers[link[1][1]], link[0]))\n",
    "            \n",
    "            if len(tmp['where']) > 0:\n",
    "                tmp['where'] = tmp['where'][0]\n",
    "            else:\n",
    "                tmp['where'] = ['*', '*']\n",
    "            \n",
    "            outputs.append({'question': d['question'], 'table_id': d['table_id'],\n",
    "                            'question_id': question_id, 'answer-text': d['answer-text'],\n",
    "                            'groundtruth': tmp, 'aggregate': ['maximum', 'minimum', 'earliest', 'latest', ''], \n",
    "                            'target': headers, 'where': potential_link})\n",
    "\n",
    "with open('Mixed-Reasoning/KBQA_step1.json', 'w') as f:\n",
    "    json.dump(outputs, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/dev_input.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "possible = 0\n",
    "outputs = []\n",
    "for d in data:\n",
    "    table_id = d['table_id']\n",
    "    with open('tables_tok/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)\n",
    "    headers = [_[0][0] for _ in table['header']]\n",
    "    \n",
    "    potential_link = []\n",
    "    for n in d['nodes']:\n",
    "        if n[-1] == 'links' and n[3] == 'string match':\n",
    "            potential_link.append((headers[n[1][1]], n[0]))\n",
    "    \n",
    "    potential_link.append(('*', '*'))\n",
    "    \n",
    "    if len(potential_link) > 1:\n",
    "        possible += 1        \n",
    "    \n",
    "    outputs.append({'question': d['question'], 'table_id': d['table_id'],\n",
    "                    'question_id': d['question_id'], 'answer-text': d['answer-text'],\n",
    "                    'aggregate': ['maximum', 'minimum', 'earliest', 'latest', ''], \n",
    "                    'target': headers, 'where': potential_link})\n",
    "\n",
    "with open('Mixed-Reasoning/dev_KBQA_step1.json', 'w') as f:\n",
    "    json.dump(outputs, f, indent=2)\n",
    "    \n",
    "print(\"upper bound of {}/{}\".format(possible, len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of the table/passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_num = 0\n",
    "row_num = 0\n",
    "cell_length = 0\n",
    "\n",
    "total = 13000\n",
    "total_cell = 0\n",
    "total_url = 0\n",
    "for i in range(0, total):\n",
    "    with open('tables_tok/{}.json'.format(i)) as f:\n",
    "        table = json.load(f)\n",
    "    column_num += len(table['header'])\n",
    "    row_num += len(table['data'])\n",
    "    \n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            content = ' , '.join(cell[0])\n",
    "            cell_length += len(content.split(' '))\n",
    "            total_cell += 1\n",
    "            \n",
    "            for url in cell[1]:\n",
    "                if url:\n",
    "                    total_url += 1\n",
    "    \n",
    "print('column num = {}; row num = {}'.format(column_num / total, row_num / total))\n",
    "print('cell num = {}; average length/cell = {}; average url/table = {}'.format(\n",
    "    total_cell / total, cell_length / total_cell, total_url / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'r') as f:\n",
    "    passages = json.load(f)\n",
    "\n",
    "vs = []\n",
    "for k, v in passages.items():\n",
    "    vs.append(tokenizer.tokenize(v)[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length, word_length = 0, 0\n",
    "for v in vs:\n",
    "    sent_length += len(v)\n",
    "    word_length += len((\" \".join(v)).split(' '))\n",
    "\n",
    "print(\"sentence length = {}, word length = {}\".format(sent_length / len(vs), word_length / len(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/processed_step3.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "length = 0\n",
    "for d in data:\n",
    "    length += len(d['question'].split(' '))\n",
    "\n",
    "print(\"avg length = {}\".format(length / len(data)))\n",
    "\n",
    "with open('Mixed-Reasoning/dev_ids.json') as f:\n",
    "    dev_ids = set(json.load(f))\n",
    "    \n",
    "with open('Mixed-Reasoning/test_ids.json') as f:\n",
    "    test_ids = set(json.load(f))\n",
    "\n",
    "tr_easy, tr_med, tr_hard, tr_cell, tr_passage = 0, 0, 0, 0, 0\n",
    "dev_easy, dev_med, dev_hard, dev_cell, dev_passage  = 0, 0, 0, 0, 0\n",
    "test_easy, test_med, test_hard, test_cell, test_passage = 0, 0, 0, 0, 0\n",
    "\n",
    "for d in data:\n",
    "    if d['question_id'] not in dev_ids and d['question_id'] not in test_ids:\n",
    "        if d['where'] == 'table':\n",
    "            tr_cell += 1\n",
    "        else:\n",
    "            tr_passage += 1\n",
    "        \n",
    "        if d['type'] == 'easy':\n",
    "            tr_easy += 1\n",
    "        elif d['type'] in ['medium', 'numeric']:\n",
    "            tr_med += 1\n",
    "        else:\n",
    "            tr_hard += 1\n",
    "    elif d['question_id'] in dev_ids:\n",
    "        if d['where'] == 'table':\n",
    "            dev_cell += 1\n",
    "        else:\n",
    "            dev_passage += 1\n",
    "        \n",
    "        if d['type'] == 'easy':\n",
    "            dev_easy += 1\n",
    "        elif d['type'] in ['medium', 'numeric']:\n",
    "            dev_med += 1\n",
    "        else:\n",
    "            dev_hard += 1\n",
    "    elif d['question_id'] in test_ids:\n",
    "        if d['where'] == 'table':\n",
    "            test_cell += 1\n",
    "        else:\n",
    "            test_passage += 1\n",
    "        \n",
    "        if d['type'] == 'easy':\n",
    "            test_easy += 1\n",
    "        elif d['type'] in ['medium', 'numeric']:\n",
    "            test_med += 1\n",
    "        else:\n",
    "            test_hard += 1\n",
    "\n",
    "print(tr_easy, tr_med, tr_hard, tr_cell, tr_passage)\n",
    "print(dev_easy, dev_med, dev_hard, dev_cell, dev_passage)\n",
    "print(test_easy, test_med, test_hard, test_cell, test_passage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscelleous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_num(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "plausible_tables = []\n",
    "for i in range(7000, 15000):\n",
    "    with open('tables/{}.json'.format(i)) as f:\n",
    "        table = json.load(f)\n",
    "    \n",
    "    for column_idx in range(len(table['header'])):      \n",
    "        if all([is_num(table['data'][row_idx][column_idx][0][0]) for row_idx in range(len(table['data']))]):    \n",
    "            plausible_tables.append(i)\n",
    "            break\n",
    "            \n",
    "print(len(plausible_tables))\n",
    "\n",
    "with open('Mixed-Reasoning/numeric_tables.json', 'w') as f:\n",
    "    json.dump(plausible_tables, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "mismatch = 0\n",
    "match = 0\n",
    "for d in data:\n",
    "    if d['where'] == 'table':\n",
    "        if d['answer-text'] != d['answer-node'][0][0]:\n",
    "            print(d['answer-node'][0][0], '#', d['answer-text'])\n",
    "            mismatch += 1\n",
    "        else:\n",
    "            match += 1\n",
    "\n",
    "print(\"final match = {}, partial match = {}\".format(match, mismatch)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
